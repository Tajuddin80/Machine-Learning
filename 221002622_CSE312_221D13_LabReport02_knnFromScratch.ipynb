{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdUJnpcwhueAeGpW6ysPh6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tajuddin80/Machine-Learning/blob/main/221002622_CSE312_221D13_LabReport02_knnFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76866663"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "\n",
        "  distance = 0.0\n",
        "  for i in range(len(point1)):\n",
        "    distance += (point1[i] - point2[i])**2\n",
        "  return np.sqrt(distance)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de9c7393"
      },
      "source": [
        "def get_neighbors(train_data, test_point, k):\n",
        "\n",
        "  distances = []\n",
        "  for train_point in train_data:\n",
        "    dist = euclidean_distance(test_point, train_point)\n",
        "    distances.append((dist, train_point))\n",
        "\n",
        "  distances.sort(key=lambda x: x[0])\n",
        "\n",
        "  neighbors = [distances[i][1] for i in range(k)]\n",
        "  return neighbors"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeb6c56c"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def predict_classification(neighbors):\n",
        "\n",
        "  classes = [neighbor[-1] for neighbor in neighbors]\n",
        "  class_counts = Counter(classes)\n",
        "  predicted_class = max(class_counts, key=class_counts.get)\n",
        "  return predicted_class"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75b96f4a"
      },
      "source": [
        "def k_nearest_neighbors(train_data, test_data, k):\n",
        "\n",
        "  predictions = []\n",
        "  for test_point in test_data:\n",
        "    neighbors = get_neighbors(train_data, test_point, k)\n",
        "    predicted_class = predict_classification(neighbors)\n",
        "    predictions.append(predicted_class)\n",
        "  return predictions"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "268dd6f1",
        "outputId": "3ea503bd-5502-4df7-a160-a6e77baf3832"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Create a sample dataset\n",
        "# Each inner list is a data point, the last element is the class label.\n",
        "dataset = [\n",
        "    [2.7810836, 2.550537003, 'A'],\n",
        "    [1.465489372, 2.362125076, 'A'],\n",
        "    [3.396561688, 4.400293529, 'A'],\n",
        "    [1.38807019, 1.850220317, 'A'],\n",
        "    [3.06407232, 3.005305971, 'A'],\n",
        "    [7.627531214, 2.759262235, 'B'],\n",
        "    [5.332441248, 2.088626775, 'B'],\n",
        "    [6.922596716, 1.77106367, 'B'],\n",
        "    [8.675418651, -0.242068655, 'B'],\n",
        "    [7.673756466, 3.508563011, 'B']\n",
        "]\n",
        "\n",
        "# 2. Split the sample dataset into training and testing sets\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.4, random_state=42)\n",
        "\n",
        "# Separate features and labels for test data for evaluation\n",
        "X_test = [point[:-1] for point in test_data]\n",
        "y_test_actual = [point[-1] for point in test_data]\n",
        "\n",
        "# 3. Call the k_nearest_neighbors function\n",
        "k = 3\n",
        "predictions = k_nearest_neighbors(train_data, X_test, k)\n",
        "\n",
        "# 4. Compare the predicted class labels with the actual class labels\n",
        "accuracy = accuracy_score(y_test_actual, predictions)\n",
        "\n",
        "print(f\"Actual class labels: {y_test_actual}\")\n",
        "print(f\"Predicted class labels: {predictions}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual class labels: ['B', 'A', 'B', 'A']\n",
            "Predicted class labels: ['B', 'A', 'B', 'A']\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d42286af"
      },
      "source": [
        "def calculate_accuracy(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates the accuracy of predictions.\n",
        "\n",
        "  Args:\n",
        "    y_true: A list of actual labels.\n",
        "    y_pred: A list of predicted labels.\n",
        "\n",
        "  Returns:\n",
        "    The accuracy of the predictions as a float.\n",
        "  \"\"\"\n",
        "  correct_predictions = 0\n",
        "  for true_label, pred_label in zip(y_true, y_pred):\n",
        "    if true_label == pred_label:\n",
        "      correct_predictions += 1\n",
        "  accuracy = correct_predictions / len(y_true)\n",
        "  return accuracy"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "169cd6a4"
      },
      "source": [
        "def generate_confusion_matrix(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Generates a confusion matrix.\n",
        "\n",
        "  Args:\n",
        "    y_true: A list of actual labels.\n",
        "    y_pred: A list of predicted labels.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary representing the confusion matrix.\n",
        "  \"\"\"\n",
        "  classes = sorted(list(set(y_true + y_pred)))\n",
        "  matrix = {true_class: {pred_class: 0 for pred_class in classes} for true_class in classes}\n",
        "\n",
        "  for true_label, pred_label in zip(y_true, y_pred):\n",
        "    matrix[true_label][pred_label] += 1\n",
        "\n",
        "  return matrix"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2c3df41"
      },
      "source": [
        "def calculate_precision(confusion_matrix, class_label):\n",
        "  \"\"\"\n",
        "  Calculates the precision for a given class.\n",
        "\n",
        "  Args:\n",
        "    confusion_matrix: A dictionary representing the confusion matrix.\n",
        "    class_label: The class label for which to calculate precision.\n",
        "\n",
        "  Returns:\n",
        "    The precision for the class as a float, or 0.0 if the denominator is zero.\n",
        "  \"\"\"\n",
        "  true_positives = confusion_matrix.get(class_label, {}).get(class_label, 0)\n",
        "  predicted_positives = sum(confusion_matrix.get(other_class, {}).get(class_label, 0) for other_class in confusion_matrix)\n",
        "  if predicted_positives == 0:\n",
        "    return 0.0\n",
        "  return true_positives / predicted_positives\n",
        "\n",
        "def calculate_recall(confusion_matrix, class_label):\n",
        "  \"\"\"\n",
        "  Calculates the recall for a given class.\n",
        "\n",
        "  Args:\n",
        "    confusion_matrix: A dictionary representing the confusion matrix.\n",
        "    class_label: The class label for which to calculate recall.\n",
        "\n",
        "  Returns:\n",
        "    The recall for the class as a float, or 0.0 if the denominator is zero.\n",
        "  \"\"\"\n",
        "  true_positives = confusion_matrix.get(class_label, {}).get(class_label, 0)\n",
        "  actual_positives = sum(confusion_matrix.get(class_label, {}).get(other_class, 0) for other_class in confusion_matrix.get(class_label, {}))\n",
        "  if actual_positives == 0:\n",
        "    return 0.0\n",
        "  return true_positives / actual_positives\n",
        "\n",
        "def calculate_f1_score(precision, recall):\n",
        "  \"\"\"\n",
        "  Calculates the F1-score.\n",
        "\n",
        "  Args:\n",
        "    precision: The precision value.\n",
        "    recall: The recall value.\n",
        "\n",
        "  Returns:\n",
        "    The F1-score as a float, or 0.0 if the sum of precision and recall is zero.\n",
        "  \"\"\"\n",
        "  if precision + recall == 0:\n",
        "    return 0.0\n",
        "  return 2 * (precision * recall) / (precision + recall)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "fba57165",
        "outputId": "fc3badcd-4198-4778-f04e-ebf209cac099"
      },
      "source": [
        "def classification_report(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Generates a comprehensive classification report including accuracy,\n",
        "    confusion matrix, precision, recall, and F1-score for each class.\n",
        "\n",
        "    Args:\n",
        "        y_true: A list of actual labels.\n",
        "        y_pred: A list of predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the classification report.\n",
        "    \"\"\"\n",
        "    report = {}\n",
        "\n",
        "    # 1. Generate confusion matrix\n",
        "    confusion_matrix = generate_confusion_matrix(y_true, y_pred)\n",
        "    report['confusion_matrix'] = confusion_matrix\n",
        "\n",
        "    # 2. Get unique class labels\n",
        "    classes = sorted(list(set(y_true + y_pred)))\n",
        "\n",
        "    # 3. Calculate metrics for each class\n",
        "    class_metrics = {}\n",
        "    for class_label in classes:\n",
        "        precision = calculate_precision(confusion_matrix, class_label)\n",
        "        recall = calculate_recall(confusion_matrix, class_label)\n",
        "        f1 = calculate_f1_score(precision, recall)\n",
        "        class_metrics[class_label] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1\n",
        "        }\n",
        "    report['class_metrics'] = class_metrics\n",
        "\n",
        "    # 6. Calculate overall accuracy\n",
        "    accuracy = calculate_accuracy(y_true, y_pred)\n",
        "    report['overall_accuracy'] = accuracy\n",
        "\n",
        "    return report\n",
        "\n",
        "# Example usage with the sample dataset from the previous cell (assuming it's available)\n",
        "# Make sure to have y_test_actual and predictions defined from the previous cell\n",
        "if 'y_test_actual' in locals() and 'predictions' in locals():\n",
        "  classification_report_output = classification_report(y_test_actual, predictions)\n",
        "  print(\"\\nClassification Report:\")\n",
        "  display(classification_report_output)\n",
        "else:\n",
        "  print(\"Please run the previous cell to get y_test_actual and predictions.\")\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'confusion_matrix': {'A': {'A': 2, 'B': 0}, 'B': {'A': 0, 'B': 2}},\n",
              " 'class_metrics': {'A': {'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0},\n",
              "  'B': {'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0}},\n",
              " 'overall_accuracy': 1.0}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "2756e75f",
        "outputId": "65d29045-4313-4e59-9b97-32bb3fcfb21b"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate scikit-learn metrics\n",
        "sklearn_accuracy = accuracy_score(y_test_actual, predictions)\n",
        "sklearn_confusion_matrix = confusion_matrix(y_test_actual, predictions)\n",
        "sklearn_precision = precision_score(y_test_actual, predictions, average=None)\n",
        "sklearn_recall = recall_score(y_test_actual, predictions, average=None)\n",
        "sklearn_f1_score = f1_score(y_test_actual, predictions, average=None)\n",
        "\n",
        "# Print custom metrics\n",
        "print(\"Custom Metrics:\")\n",
        "display(classification_report_output)\n",
        "\n",
        "# Print scikit-learn metrics\n",
        "print(\"\\nScikit-learn Metrics:\")\n",
        "print(f\"Accuracy: {sklearn_accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "display(sklearn_confusion_matrix)\n",
        "\n",
        "# Map class labels to precision, recall, f1-score from sklearn output for easier comparison\n",
        "classes = sorted(list(set(y_test_actual + predictions)))\n",
        "sklearn_class_metrics = {}\n",
        "for i, class_label in enumerate(classes):\n",
        "    sklearn_class_metrics[class_label] = {\n",
        "        'precision': sklearn_precision[i],\n",
        "        'recall': sklearn_recall[i],\n",
        "        'f1_score': sklearn_f1_score[i]\n",
        "    }\n",
        "\n",
        "print(\"Class Metrics (Precision, Recall, F1-score):\")\n",
        "display(sklearn_class_metrics)\n",
        "\n",
        "# Compare the results (visual inspection from the output)\n",
        "print(\"\\nComparison:\")\n",
        "print(\"Compare the 'Custom Metrics' and 'Scikit-learn Metrics' outputs above to ensure correctness.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Metrics:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'confusion_matrix': {'A': {'A': 2, 'B': 0}, 'B': {'A': 0, 'B': 2}},\n",
              " 'class_metrics': {'A': {'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0},\n",
              "  'B': {'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0}},\n",
              " 'overall_accuracy': 1.0}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scikit-learn Metrics:\n",
            "Accuracy: 1.0\n",
            "Confusion Matrix:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[2, 0],\n",
              "       [0, 2]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Metrics (Precision, Recall, F1-score):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'A': {'precision': np.float64(1.0),\n",
              "  'recall': np.float64(1.0),\n",
              "  'f1_score': np.float64(1.0)},\n",
              " 'B': {'precision': np.float64(1.0),\n",
              "  'recall': np.float64(1.0),\n",
              "  'f1_score': np.float64(1.0)}}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison:\n",
            "Compare the 'Custom Metrics' and 'Scikit-learn Metrics' outputs above to ensure correctness.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# -----------------------\n",
        "# Step 1: KNN Algorithm\n",
        "# -----------------------\n",
        "class KNNClassifier:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = np.array(X_train)\n",
        "        self.y_train = np.array(y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = [self._predict_point(x) for x in X_test]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict_point(self, x):\n",
        "        distances = [np.linalg.norm(x - train_x) for train_x in self.X_train]\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = self.y_train[k_indices]\n",
        "        return Counter(k_nearest_labels).most_common(1)[0][0]\n",
        "\n",
        "# -----------------------\n",
        "# Step 2: Custom Metrics\n",
        "# -----------------------\n",
        "def custom_metrics(y_true, y_pred):\n",
        "    labels = np.unique(y_true)\n",
        "    results = {}\n",
        "\n",
        "    for label in labels:\n",
        "        TP = np.sum((y_pred == label) & (y_true == label))\n",
        "        FP = np.sum((y_pred == label) & (y_true != label))\n",
        "        FN = np.sum((y_pred != label) & (y_true == label))\n",
        "\n",
        "        precision = TP / (TP + FP) if TP + FP else 0\n",
        "        recall = TP / (TP + FN) if TP + FN else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall else 0\n",
        "\n",
        "        results[label] = {\n",
        "            'Precision': round(precision, 4),\n",
        "            'Recall': round(recall, 4),\n",
        "            'F1 Score': round(f1, 4)\n",
        "        }\n",
        "\n",
        "    accuracy = np.mean(y_true == y_pred)\n",
        "    results['Accuracy'] = round(accuracy, 4)\n",
        "\n",
        "    return results\n",
        "\n",
        "# -----------------------\n",
        "# Step 3: Iris Dataset\n",
        "# -----------------------\n",
        "def run_iris_classification():\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    model = KNNClassifier(k=3)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(\" Iris Dataset Evaluation:\")\n",
        "    print(custom_metrics(y_test, y_pred))\n",
        "\n",
        "# -----------------------\n",
        "# Step 4: News Dataset Example\n",
        "# -----------------------\n",
        "def run_news_classification():\n",
        "    # Sample News Dataset\n",
        "    data = {\n",
        "        'title': [\n",
        "            \"Stocks fall amid inflation fears\",\n",
        "            \"Champions League: Madrid wins again\",\n",
        "            \"Scientists discover new exoplanet\",\n",
        "            \"New iPhone released this week\",\n",
        "            \"Elections coming up next month\"\n",
        "        ],\n",
        "        'category': ['business', 'sports', 'science', 'tech', 'politics']\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Convert text to numerical vectors (simple representation using word counts)\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(df['title']).toarray()\n",
        "\n",
        "    label_enc = LabelEncoder()\n",
        "    y = label_enc.fit_transform(df['category'])\n",
        "\n",
        "    # Split into train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "    model = KNNClassifier(k=3)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"\\n News Dataset Evaluation:\")\n",
        "    print(custom_metrics(y_test, y_pred))\n",
        "\n",
        "# -----------------------\n",
        "# Run Both\n",
        "# -----------------------\n",
        "run_iris_classification()\n",
        "run_news_classification()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub4UWcGAGyFd",
        "outputId": "4b8ee335-7aa9-43ad-bdd5-b33f4454ede6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Iris Dataset Evaluation:\n",
            "{np.int64(0): {'Precision': np.float64(1.0), 'Recall': np.float64(1.0), 'F1 Score': np.float64(1.0)}, np.int64(1): {'Precision': np.float64(1.0), 'Recall': np.float64(1.0), 'F1 Score': np.float64(1.0)}, np.int64(2): {'Precision': np.float64(1.0), 'Recall': np.float64(1.0), 'F1 Score': np.float64(1.0)}, 'Accuracy': np.float64(1.0)}\n",
            "\n",
            " News Dataset Evaluation:\n",
            "{np.int64(1): {'Precision': 0, 'Recall': np.float64(0.0), 'F1 Score': 0}, np.int64(3): {'Precision': 0, 'Recall': np.float64(0.0), 'F1 Score': 0}, 'Accuracy': np.float64(0.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# -------------------------\n",
        "# Custom KNN Implementation\n",
        "# -------------------------\n",
        "class CustomKNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = np.array(X_train)\n",
        "        self.y_train = np.array(y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return np.array([self._predict_single(x) for x in X_test])\n",
        "\n",
        "    def _predict_single(self, x):\n",
        "        distances = [np.linalg.norm(x - train_x) for train_x in self.X_train]\n",
        "        nearest_indices = np.argsort(distances)[:self.k]\n",
        "        nearest_labels = self.y_train[nearest_indices]\n",
        "        return Counter(nearest_labels).most_common(1)[0][0]\n",
        "\n",
        "# -------------------------\n",
        "# Evaluation Function\n",
        "# -------------------------\n",
        "def evaluate(y_true, y_pred):\n",
        "    return {\n",
        "        'accuracy': round(accuracy_score(y_true, y_pred), 4),\n",
        "        'precision_macro': round(precision_score(y_true, y_pred, average='macro'), 4),\n",
        "        'recall_macro': round(recall_score(y_true, y_pred, average='macro'), 4),\n",
        "        'f1_macro': round(f1_score(y_true, y_pred, average='macro'), 4)\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# Test with Various k and Split Ratios\n",
        "# -------------------------\n",
        "def compare_knn_models():\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "\n",
        "    best_config = None\n",
        "    best_custom = None\n",
        "    best_sklearn = None\n",
        "    best_score = 0\n",
        "\n",
        "    for test_size in [0.2, 0.3, 0.4]:\n",
        "        for k in range(1, 11):\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "            # Custom KNN\n",
        "            custom_knn = CustomKNN(k=k)\n",
        "            custom_knn.fit(X_train, y_train)\n",
        "            y_pred_custom = custom_knn.predict(X_test)\n",
        "            custom_scores = evaluate(y_test, y_pred_custom)\n",
        "\n",
        "            # Sklearn KNN\n",
        "            sk_knn = KNeighborsClassifier(n_neighbors=k)\n",
        "            sk_knn.fit(X_train, y_train)\n",
        "            y_pred_sklearn = sk_knn.predict(X_test)\n",
        "            sklearn_scores = evaluate(y_test, y_pred_sklearn)\n",
        "\n",
        "            # Compare by F1 (macro)\n",
        "            avg_f1 = (custom_scores['f1_macro'] + sklearn_scores['f1_macro']) / 2\n",
        "            if avg_f1 > best_score:\n",
        "                best_score = avg_f1\n",
        "                best_config = {'k': k, 'test_size': test_size}\n",
        "                best_custom = custom_scores\n",
        "                best_sklearn = sklearn_scores\n",
        "\n",
        "    # Final Output\n",
        "    print(f\" Best Config → k = {best_config['k']}, Test Size = {best_config['test_size']}\\n\")\n",
        "\n",
        "    print(\" Custom KNN Evaluation:\")\n",
        "    for k, v in best_custom.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    print(\"\\n Scikit-learn KNN Evaluation:\")\n",
        "    for k, v in best_sklearn.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "# Run Comparison\n",
        "compare_knn_models()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic8-eDyF3XhP",
        "outputId": "8b497a2d-9de1-411b-a574-f3e5c4a6b0a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Best Config → k = 1, Test Size = 0.2\n",
            "\n",
            " Custom KNN Evaluation:\n",
            "accuracy: 1.0\n",
            "precision_macro: 1.0\n",
            "recall_macro: 1.0\n",
            "f1_macro: 1.0\n",
            "\n",
            " Scikit-learn KNN Evaluation:\n",
            "accuracy: 1.0\n",
            "precision_macro: 1.0\n",
            "recall_macro: 1.0\n",
            "f1_macro: 1.0\n"
          ]
        }
      ]
    }
  ]
}